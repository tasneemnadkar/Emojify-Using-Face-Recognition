{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a685bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a56400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the convolution network architecture\n",
    "\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "face_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Flatten())\n",
    "face_model.add(Dense(1024, activation='relu'))\n",
    "face_model.add(Dropout(0.5))\n",
    "face_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved weights\n",
    "\n",
    "face_model.load_weights('recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeed557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable OpenCL\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df98dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets Dictionaries\n",
    "\n",
    "facial_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "emojis_dict = {0:\"emojis/angry.png\", 1:\"emojis/disgusted.png\", 2:\"emojis/fearful.png\", 3:\"emojis/happy.png\", 4:\"emojis/neutral.png\", 5:\"emojis/sad.png\", 6:\"emojis/surprised.png\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c56df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for \n",
    "\n",
    "global last_frame1                         \n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get face captured and recognize emotion\n",
    "\n",
    "def Capture_Image():\n",
    "    global cap1\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"Unable to open the camera !\")\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    # It will detect the face in the video and bound it with a rectangular box\n",
    "    bound_box = cv2.CascadeClassifier('haarcascades_cuda/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    n_faces = bound_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in n_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_frame = gray_frame[y:y + h, x:x + w]\n",
    "        crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "        prediction = face_model.predict(crop_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "\n",
    "    if flag1 is None:\n",
    "        print (\"Error!\")\n",
    "\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB) #to store the image   \n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, Capture_Image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b62a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for showing Emoji According to Facial Expression\n",
    "\n",
    "def Get_Emoji():\n",
    "    frame2=cv2.imread(emojis_dict[show_text[0]])\n",
    "    pic2=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "    img2=Image.fromarray(frame2)\n",
    "    imgtk2=ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2=imgtk2\n",
    "    lmain3.configure(text=facial_dict[show_text[0]],font=('arial',45,'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    lmain2.after(10, Get_Emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90634c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI Window to show captured image with emoji\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root=tk.Tk()  \n",
    "    heading = Label(root,bg='black')\n",
    "    heading.pack()\n",
    "    # To label the output\n",
    "    heading2=Label(root,text=\"Emojify\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')                                \n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=350)\n",
    "    root.title(\"Emojify\")           \n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    Capture_Image()\n",
    "    Get_Emoji()\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
